{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "\n",
    "In this notebook, we illustrate how to use the Neural News Recommendation with Multi-Head Self-Attention ([NRMS](https://aclanthology.org/D19-1671/)). The implementation is taken from the [recommenders](https://github.com/recommenders-team/recommenders) repository. We have simply stripped the model to keep it cleaner.\n",
    "\n",
    "We use a small dataset, which is downloaded from [recsys.eb.dk](https://recsys.eb.dk/). All the datasets are stored in the folder path ```~/ebnerd_data/*```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from ebrec.evaluation import MetricEvaluator, AucScore, NdcgScore, MrrScore\n",
    "from ebrec.models.newsrec.model_config import hparams_nrms, hparams_naml\n",
    "from ebrec.models.newsrec.naml import NAMLModel\n",
    "from ebrec.utils._articles import (\n",
    "    create_article_id_to_value_mapping,\n",
    "    convert_text2encoding_with_transformers\n",
    ")\n",
    "from ebrec.utils._behaviors import (\n",
    "    create_binary_labels_column,\n",
    "    sampling_strategy_wu2019,\n",
    "    add_known_user_column,\n",
    "    add_prediction_scores,\n",
    "    truncate_history,\n",
    "    create_user_id_to_int_mapping\n",
    ")\n",
    "from ebrec.utils._constants import (\n",
    "    DEFAULT_ARTICLE_ID_COL,\n",
    "    DEFAULT_CATEGORY_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_SUBTITLE_COL,\n",
    "    DEFAULT_LABELS_COL,\n",
    "    DEFAULT_TITLE_COL,\n",
    "    DEFAULT_USER_COL,\n",
    ")\n",
    "from ebrec.utils._nlp import get_transformers_word_embeddings\n",
    "from ebrec.utils._polars import concat_str_columns, slice_join_dataframes\n",
    "from ebrec.utils._python import create_lookup_dict, time_it, write_submission_file, rank_predictions_by_score, create_lookup_objects\n",
    "from ebrec.utils._articles_behaviors import map_list_article_id_to_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate the dataloaders\n",
    "In the implementations we have disconnected the models and data. Hence, you should built a dataloader that fits your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NewsrecDataLoader(tf.keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    A DataLoader for news recommendation.\n",
    "    \"\"\"\n",
    "\n",
    "    behaviors: pl.DataFrame\n",
    "    history_column: str\n",
    "    article_dict: dict[int, any]\n",
    "    unknown_representation: str\n",
    "    eval_mode: bool = False\n",
    "    batch_size: int = 32\n",
    "    inview_col: str = DEFAULT_INVIEW_ARTICLES_COL\n",
    "    labels_col: str = DEFAULT_LABELS_COL\n",
    "    user_col: str = DEFAULT_USER_COL\n",
    "    kwargs: field(default_factory=dict) = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"\n",
    "        Post-initialization method. Loads the data and sets additional attributes.\n",
    "        \"\"\"\n",
    "        self.lookup_article_index, self.lookup_article_matrix = create_lookup_objects(\n",
    "            self.article_dict, unknown_representation=self.unknown_representation\n",
    "        )\n",
    "        self.unknown_index = [0]\n",
    "        self.X, self.y = self.load_data()\n",
    "        if self.kwargs is not None:\n",
    "            self.set_kwargs(self.kwargs)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return int(np.ceil(len(self.X) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self):\n",
    "        raise ValueError(\"Function '__getitem__' needs to be implemented.\")\n",
    "\n",
    "    def load_data(self) -> tuple[pl.DataFrame, pl.DataFrame]:\n",
    "        X = self.behaviors.drop(self.labels_col).with_columns(\n",
    "            pl.col(self.inview_col).list.len().alias(\"n_samples\")\n",
    "        )\n",
    "        y = self.behaviors[self.labels_col]\n",
    "        return X, y\n",
    "\n",
    "    def set_kwargs(self, kwargs: dict):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "\n",
    "@dataclass(kw_only=True)\n",
    "class NAMLDataLoader(NewsrecDataLoader):\n",
    "    \"\"\"\n",
    "    Eval mode not implemented\n",
    "    \"\"\"\n",
    "\n",
    "    unknown_category_value: int = 0\n",
    "    unknown_subcategory_value: int = 0\n",
    "    body_mapping: dict[int, list[int]] = None\n",
    "    category_mapping: dict[int, int] = None\n",
    "    subcategory_mapping: dict[int, int] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.title_prefix = \"title_\"\n",
    "        self.body_prefix = \"body_\"\n",
    "        self.category_prefix = \"category_\"\n",
    "        self.subcategory_prefix = \"subcategory_\"\n",
    "        (\n",
    "            self.lookup_article_index_body,\n",
    "            self.lookup_article_matrix_body,\n",
    "        ) = create_lookup_objects(\n",
    "            self.body_mapping, unknown_representation=self.unknown_representation\n",
    "        )\n",
    "        # if self.eval_mode:\n",
    "        #     raise ValueError(\"'eval_mode = True' is not implemented for NAML\")\n",
    "\n",
    "        return super().__post_init__()\n",
    "\n",
    "    def transform(self, df: pl.DataFrame) -> tuple[pl.DataFrame]:\n",
    "        \"\"\"\n",
    "        Special case for NAML as it requires body-encoding, verticals, & subvertivals\n",
    "        \"\"\"\n",
    "        # =>\n",
    "        title = df.pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=self.history_column,\n",
    "            mapping=self.lookup_article_index,\n",
    "            fill_nulls=self.unknown_index,\n",
    "            drop_nulls=False,\n",
    "        ).pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=self.inview_col,\n",
    "            mapping=self.lookup_article_index,\n",
    "            fill_nulls=self.unknown_index,\n",
    "            drop_nulls=False,\n",
    "        )\n",
    "        # =>\n",
    "        body = df.pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=self.history_column,\n",
    "            mapping=self.lookup_article_index_body,\n",
    "            fill_nulls=self.unknown_index,\n",
    "            drop_nulls=False,\n",
    "        ).pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=self.inview_col,\n",
    "            mapping=self.lookup_article_index_body,\n",
    "            fill_nulls=self.unknown_index,\n",
    "            drop_nulls=False,\n",
    "        )\n",
    "        # =>\n",
    "        category = df.pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=self.history_column,\n",
    "            mapping=self.category_mapping,\n",
    "            fill_nulls=self.unknown_category_value,\n",
    "            drop_nulls=False,\n",
    "        ).pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=self.inview_col,\n",
    "            mapping=self.category_mapping,\n",
    "            fill_nulls=self.unknown_category_value,\n",
    "            drop_nulls=False,\n",
    "        )\n",
    "        # =>\n",
    "        subcategory = df.pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=self.history_column,\n",
    "            mapping=self.subcategory_mapping,\n",
    "            fill_nulls=self.unknown_subcategory_value,\n",
    "            drop_nulls=False,\n",
    "        ).pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=self.inview_col,\n",
    "            mapping=self.subcategory_mapping,\n",
    "            fill_nulls=self.unknown_subcategory_value,\n",
    "            drop_nulls=False,\n",
    "        )\n",
    "        return (\n",
    "            pl.DataFrame()\n",
    "            .with_columns(title.select(pl.all().name.prefix(self.title_prefix)))\n",
    "            .with_columns(body.select(pl.all().name.prefix(self.body_prefix)))\n",
    "            .with_columns(category.select(pl.all().name.prefix(self.category_prefix)))\n",
    "            .with_columns(\n",
    "                subcategory.select(pl.all().name.prefix(self.subcategory_prefix))\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def __getitem__(self, idx) -> tuple[tuple[np.ndarray], np.ndarray]:\n",
    "        batch_X = self.X[idx * self.batch_size : (idx + 1) * self.batch_size].pipe(\n",
    "            self.transform\n",
    "        )\n",
    "        \n",
    "        batch_y = self.y[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        # =>\n",
    "        \n",
    "        batch_y = np.array(batch_y.to_list())\n",
    "        his_input_title = np.array(batch_X[self.title_prefix + self.history_column].to_list())\n",
    "        his_input_body = np.array(batch_X[self.body_prefix + self.history_column].to_list())\n",
    "\n",
    "        # =>\n",
    "        pred_input_title = np.array(batch_X[self.title_prefix + self.inview_col].to_list())   \n",
    "        pred_input_body = np.array(batch_X[self.body_prefix + self.inview_col].to_list())\n",
    "\n",
    "        # =>\n",
    "        his_input_title = np.squeeze(self.lookup_article_matrix[his_input_title], axis=2)\n",
    "        pred_input_title = np.squeeze(self.lookup_article_matrix[pred_input_title], axis=2)\n",
    "        his_input_body = np.squeeze(self.lookup_article_matrix_body[his_input_body], axis=2)\n",
    "        pred_input_body = np.squeeze(self.lookup_article_matrix_body[pred_input_body], axis=2)\n",
    "        # =>\n",
    "\n",
    "        his_input_vert = np.array(batch_X[self.category_prefix + self.history_column].to_list())[:, :, np.newaxis]\n",
    "        his_input_subvert = np.array(batch_X[self.subcategory_prefix + self.history_column].to_list())[:, :, np.newaxis]\n",
    "        pred_input_vert = np.array(batch_X[self.category_prefix + self.inview_col].to_list())[:, :, np.newaxis]\n",
    "        pred_input_subvert = np.array(batch_X[self.subcategory_prefix + self.inview_col].to_list())[:, :, np.newaxis]\n",
    "\n",
    "        if self.eval_mode: # Added the eval_mode condition\n",
    "            return (\n",
    "                his_input_title,\n",
    "                his_input_body,\n",
    "                his_input_vert,\n",
    "                his_input_subvert,\n",
    "                pred_input_title,\n",
    "                pred_input_body,\n",
    "                pred_input_vert,\n",
    "                pred_input_subvert,\n",
    "            ), \n",
    "        else:\n",
    "            return (\n",
    "                his_input_title,\n",
    "                his_input_body,\n",
    "                his_input_vert,\n",
    "                his_input_subvert,\n",
    "                pred_input_title,\n",
    "                pred_input_body,\n",
    "                pred_input_vert,\n",
    "                pred_input_subvert,\n",
    "            ), batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from collections import Counter  \n",
    "def make_data_loader(PATH_DATA, do_eval):\n",
    "        \n",
    "    TOKEN_COL = \"tokens\"\n",
    "    N_SAMPLES = \"n\"\n",
    "    BATCH_SIZE = 100\n",
    "    df_articles = (\n",
    "        pl.scan_parquet(PATH_DATA.joinpath(\"../articles.parquet\"))\n",
    "        .select(pl.col(DEFAULT_ARTICLE_ID_COL, DEFAULT_CATEGORY_COL))\n",
    "        .with_columns(pl.Series(TOKEN_COL, np.random.randint(0, 20, (1, 10))))\n",
    "        .collect()\n",
    "    )\n",
    "    df_history = (\n",
    "        pl.scan_parquet(PATH_DATA.joinpath(\"history.parquet\"))\n",
    "        .select(DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL)\n",
    "        .with_columns(pl.col(DEFAULT_HISTORY_ARTICLE_ID_COL).list.tail(3))\n",
    "    )\n",
    "    df_behaviors = (\n",
    "        pl.scan_parquet(PATH_DATA.joinpath(\"behaviors.parquet\"))\n",
    "        .select(DEFAULT_USER_COL, DEFAULT_INVIEW_ARTICLES_COL, DEFAULT_CLICKED_ARTICLES_COL)\n",
    "        .with_columns(pl.col(DEFAULT_INVIEW_ARTICLES_COL).list.len().alias(N_SAMPLES))\n",
    "        .join(df_history, on=DEFAULT_USER_COL, how=\"left\")\n",
    "        .collect()\n",
    "        .pipe(create_binary_labels_column)\n",
    "    )\n",
    "    # => MAPPINGS:\n",
    "    article_mapping = create_article_id_to_value_mapping(\n",
    "        df=df_articles, value_col=TOKEN_COL\n",
    "    )\n",
    "    user_mapping = create_user_id_to_int_mapping(df=df_behaviors)\n",
    "    # => NPRATIO IMPRESSION - SAME LENGTHS:\n",
    "    #df_behaviors_train = df_behaviors.filter(pl.col(N_SAMPLES) == pl.col(N_SAMPLES).min())\n",
    "\n",
    "    min_length = 10000\n",
    "    for i in range(len(df_behaviors[\"article_ids_inview\"])):\n",
    "        if len(df_behaviors[\"article_ids_inview\"][i]) < min_length:\n",
    "            min_length = len(df_behaviors[\"article_ids_inview\"][i])\n",
    "\n",
    "    article_ids_inview = []\n",
    "    labels = []\n",
    "    user_id = []\n",
    "    article_ids_clicked = []\n",
    "    article_id_fixed = []\n",
    "    n = []\n",
    "    count = 0\n",
    "    for i in range(len(df_behaviors[\"article_ids_inview\"])):\n",
    "        full_article_ids_inview = df_behaviors[\"article_ids_inview\"][i].to_list()\n",
    "        full_labels = df_behaviors[\"labels\"][i].to_list()\n",
    "        full_article_ids_clicked = df_behaviors[\"article_ids_clicked\"][i].to_list()\n",
    "        full_article_id_fixed = df_behaviors[\"article_id_fixed\"][i].to_list()\n",
    "        \n",
    "        if not full_article_ids_inview or not full_labels or not full_article_ids_clicked or not full_article_id_fixed:\n",
    "          continue  # Skip the rest of the loop for this iteration\n",
    "\n",
    "        if len(full_article_ids_inview) == min_length:\n",
    "            article_ids_inview.append(full_article_ids_inview)\n",
    "            labels.append(full_labels)\n",
    "            article_ids_clicked.append(full_article_ids_clicked)\n",
    "            article_id_fixed.append(full_article_id_fixed)\n",
    "            user_id.append(df_behaviors[\"user_id\"][i])\n",
    "            n.append(df_behaviors[\"n\"][i])\n",
    "        else:\n",
    "            try:\n",
    "                for id in full_article_ids_clicked:\n",
    "                    if id in full_article_ids_inview:\n",
    "                        full_article_ids_inview.remove(id)\n",
    "            except ValueError as e:\n",
    "                continue\n",
    "            if round((len(full_article_ids_clicked)/len(full_article_ids_inview)) * min_length) >= 1:\n",
    "                count += 1\n",
    "            amount_to_remove = round((len(full_article_ids_clicked)/len(full_article_ids_inview)) * min_length)\n",
    "            article_ids_inview.append(full_article_ids_inview[:(min_length-amount_to_remove)] + full_article_ids_clicked[:amount_to_remove])\n",
    "            labels.append([0]*(min_length-amount_to_remove) + [1] * amount_to_remove)\n",
    "            article_ids_clicked.append(full_article_ids_clicked[:amount_to_remove])\n",
    "            article_id_fixed.append(full_article_id_fixed)\n",
    "            user_id.append(df_behaviors[\"user_id\"][i])\n",
    "            n.append(df_behaviors[\"n\"][i])\n",
    "\n",
    "            # Generate a permutation index (Shuffle the data)\n",
    "            if len(labels[-1]) != len(article_ids_inview[-1]):\n",
    "                #print(\"Length of labels and article_ids_inview is not the same\")\n",
    "                continue\n",
    "            indices = np.arange(len(labels[-1]))\n",
    "            np.random.shuffle(indices)\n",
    "            article_ids_inview[-1] = [article_ids_inview[-1][i] for i in indices]\n",
    "            labels[-1] = [labels[-1][i] for i in indices]\n",
    "        \n",
    "        if len(article_ids_inview[-1]) != min_length:\n",
    "            article_ids_inview.pop()\n",
    "            labels.pop()\n",
    "            article_ids_clicked.pop()\n",
    "            article_id_fixed.pop()\n",
    "            user_id.pop()\n",
    "            n.pop()\n",
    "        assert len(article_ids_inview[-1]) == min_length, print(\"Lenght is\" , len(article_ids_inview[-1]))\n",
    "\n",
    "    df_behaviors = pd.DataFrame({\"user_id\": user_id, \"article_ids_inview\": article_ids_inview, \"article_ids_clicked\": article_ids_clicked,\n",
    "                                 \"labels\": labels, \"n\": n, \"article_id_fixed\":article_id_fixed})\n",
    "    # Apply padding safely to each list\n",
    "    df_behaviors_train = pl.from_pandas(df_behaviors)\n",
    "    print(\"Length of the dataset\", count)\n",
    "    # Calculate the lengths of each list\n",
    "    lengths = [len(x) for x in df_behaviors_train['article_ids_clicked']]\n",
    "\n",
    "    # Count occurrences of each length\n",
    "    length_counts = Counter(lengths)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Lengths of 1's in the sequences:\")\n",
    "    for length, count in sorted(length_counts.items()):\n",
    "        print(f\"len {length} = {count}\")\n",
    "    # => MAPPINGS:\n",
    "    body_mapping = article_mapping\n",
    "    category_mapping = create_lookup_dict(\n",
    "        df_articles.select(pl.col(DEFAULT_CATEGORY_COL).unique()).with_row_index(\n",
    "            \"row_nr\"\n",
    "        ),\n",
    "        key=DEFAULT_CATEGORY_COL,\n",
    "        value=\"row_nr\",\n",
    "    )\n",
    "    subcategory_mapping = category_mapping\n",
    "\n",
    "    dataloader = NAMLDataLoader(\n",
    "        behaviors=df_behaviors_train,\n",
    "        eval_mode=do_eval,\n",
    "        article_dict=article_mapping,\n",
    "        body_mapping=body_mapping,\n",
    "        category_mapping=category_mapping,\n",
    "        unknown_representation=\"zeros\",\n",
    "        subcategory_mapping=subcategory_mapping,\n",
    "        history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "\n",
    "    return dataloader, df_behaviors_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataset 11255\n",
      "Lengths of 1's in the sequences:\n",
      "len 0 = 9111\n",
      "len 1 = 15549\n",
      "len 2 = 64\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATA:\n",
    "PATH_DATA = Path(\"C:/Users/jortv/OneDrive/Bureau/ONCE/ebnerd-benchmark/data/ebnerd_demo/train\")\n",
    "do_eval = False\n",
    "train_dataloader, df_behaviors_train = make_data_loader(PATH_DATA, do_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataset 11473\n",
      "Lengths of 1's in the sequences:\n",
      "len 0 = 9852\n",
      "len 1 = 15443\n",
      "len 2 = 60\n",
      "len 3 = 1\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATA:\n",
    "PATH_DATA = Path(\"C:/Users/jortv/OneDrive/Bureau/ONCE/ebnerd-benchmark/data/ebnerd_demo/validation\")\n",
    "do_eval = True\n",
    "val_dataloader, df_behaviors_val = make_data_loader(PATH_DATA, do_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_TITLE_SIZE = 10 #30\n",
    "DEFAULT_BODY_SIZE = 10 #40\n",
    "UNKNOWN_TITLE_VALUE = [0] * DEFAULT_TITLE_SIZE\n",
    "UNKNOWN_BODY_VALUE = [0] * DEFAULT_BODY_SIZE\n",
    "\n",
    "DEFAULT_DOCUMENT_SIZE = 768\n",
    "\n",
    "\n",
    "class hparams_naml:\n",
    "    # INPUT DIMENTIONS:\n",
    "    title_size: int = DEFAULT_TITLE_SIZE\n",
    "    history_size: int = 50 #3 #50\n",
    "    body_size: int = DEFAULT_BODY_SIZE\n",
    "    vert_num: int = 100\n",
    "    vert_emb_dim: int = 10\n",
    "    subvert_num: int = 100\n",
    "    subvert_emb_dim: int = 10\n",
    "    # MODEL ARCHITECTURE\n",
    "    dense_activation: str = \"relu\"\n",
    "    cnn_activation: str = \"relu\"\n",
    "    attention_hidden_dim: int = 150\n",
    "    filter_num: int = 400\n",
    "    window_size: int = 5\n",
    "    # MODEL OPTIMIZER:\n",
    "    optimizer: str = \"adam\"\n",
    "    loss: str = \"cross_entropy_loss\"\n",
    "    dropout: float = 0.2\n",
    "    learning_rate: float = 0.001\n",
    "\n",
    "\n",
    "config = hparams_naml()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at FacebookAI/xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "248/248 [==============================] - 203s 798ms/step - loss: 1.0195\n",
      "Epoch 2/20\n",
      "248/248 [==============================] - 199s 802ms/step - loss: 1.0195\n",
      "Epoch 3/20\n",
      " 60/248 [======>.......................] - ETA: 2:31 - loss: 1.0228"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m word2vec_embedding \u001b[38;5;241m=\u001b[39m get_transformers_word_embeddings(transformer_model)\n\u001b[0;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m NAMLModel(hparams\u001b[38;5;241m=\u001b[39mconfig, word2vec_embedding\u001b[38;5;241m=\u001b[39mword2vec_embedding)\n\u001b[1;32m----> 7\u001b[0m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfit(train_dataloader, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m ):\n\u001b[0;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    869\u001b[0m       args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_config\n\u001b[0;32m    870\u001b[0m   )\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1487\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1488\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1489\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1490\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1491\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1492\u001b[0m   )\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model\n",
    "TRANSFORMER_MODEL_NAME = \"FacebookAI/xlm-roberta-base\"\n",
    "transformer_model = AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "word2vec_embedding = get_transformers_word_embeddings(transformer_model)\n",
    "\n",
    "model = NAMLModel(hparams=config, word2vec_embedding=word2vec_embedding)\n",
    "model.model.fit(train_dataloader, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example how to compute some metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/254 [=============>................] - ETA: 9s "
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (100,) + inhomogeneous part.\nTraceback (most recent call last):\n\n  File \"c:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 270, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"c:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py\", line 917, in wrapped_generator\n    for data in generator_fn():\n\n  File \"c:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py\", line 1064, in generator_fn\n    yield x[i]\n          ~^^^\n\n  File \"C:\\Users\\jortv\\AppData\\Local\\Temp\\ipykernel_14660\\1611686830.py\", line 159, in __getitem__\n    pred_input_title = np.array(batch_X[self.title_prefix + self.inview_col].to_list())\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (100,) + inhomogeneous part.\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_predict_function_67200]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pred_validation \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(val_dataloader)\n",
      "File \u001b[1;32mc:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (100,) + inhomogeneous part.\nTraceback (most recent call last):\n\n  File \"c:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 270, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"c:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py\", line 917, in wrapped_generator\n    for data in generator_fn():\n\n  File \"c:\\Users\\jortv\\anaconda3\\envs\\baseline_rs\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py\", line 1064, in generator_fn\n    yield x[i]\n          ~^^^\n\n  File \"C:\\Users\\jortv\\AppData\\Local\\Temp\\ipykernel_14660\\1611686830.py\", line 159, in __getitem__\n    pred_input_title = np.array(batch_X[self.title_prefix + self.inview_col].to_list())\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (100,) + inhomogeneous part.\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_predict_function_67200]"
     ]
    }
   ],
   "source": [
    "pred_validation = model.model.predict(val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MetricEvaluator class>: \n",
       " {\n",
       "    \"auc\": 0.4788774973711882,\n",
       "    \"mrr\": 0.8613661475806585,\n",
       "    \"ndcg@5\": 0.9467854160632154,\n",
       "    \"ndcg@10\": 0.9467854160632154\n",
       "}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df_validation = add_prediction_scores(df_behaviors_val, pred_validation.tolist()).pipe(\n",
    "    add_known_user_column, known_users=df_behaviors_val[DEFAULT_USER_COL]\n",
    ")\n",
    "\n",
    "metrics = MetricEvaluator(\n",
    "    labels=df_validation[\"labels\"].to_list(),\n",
    "    predictions=df_validation[\"scores\"].to_list(),\n",
    "    metric_functions=[AucScore(), MrrScore(), NdcgScore(k=5), NdcgScore(k=10)],\n",
    ")\n",
    "metrics.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
